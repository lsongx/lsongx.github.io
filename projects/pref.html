<!-- 
    Author: Liangchen Song
    Author URL: https://lsongx.github.io/
    License: Creative Commons Attribution 3.0 Unported
    License URL: http://creativecommons.org/licenses/by/3.0/
-->

<head>
    <!-- <meta http-equiv="refresh" content="2; url='http://pref.uiius.com'" /> -->
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>PREF: Predictability Regularized Neural Motion Fields</title>
    <meta name="author" content="Liangchen Song">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Bungee+Shade&family=Titillium+Web&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../style.css">
</head>


<body>
<div class="main" style="max-width: 1080px;">
<div class="content">
<table>
<tbody>
<tr>
    <td style="font-size: 250%; font-weight: bold"><span style="font-family:'Bungee Shade';">PREF</span>: Predictability Regularized Neural Motion Fields</td>
</tr>
<tr>
    <td>
        <p style="font-size: 120%;">
        <a href="../index.html">Liangchen Song</a><sup style="color: #cb4b16;">1</sup>, 
        <a href="https://xixigong.github.io/">Xuan Gong</a><sup style="color: #cb4b16;">1</sup>, 
        <a href="https://planche.me/">Benjamin Planche</a><sup style="color: #859900;">2</sup>, 
        <a href="https://scholar.google.com/citations?user=1D5PfMgAAAAJ&hl=en">Meng Zheng</a><sup style="color: #859900;">2</sup>,
        </p>
        <p style="font-size: 120%;">
        <a href="https://cse.buffalo.edu/~doermann/">David Doermann</a><sup style="color: #cb4b16;">1</sup>, 
        <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a><sup style="color: #cb4b16;">1</sup>, 
        <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a><sup style="color: #859900;">2</sup>, and 
        <a href="http://wuziyan.com/">Ziyan Wu</a><sup style="color: #859900;">2</sup>
        </p>
        <p style="font-size: 90%;"><sup style="color: #cb4b16;">1</sup> University at Buffalo, Buffalo NY, USA</p>
        <p style="font-size: 90%;"><sup style="color: #859900;">2</sup> United Imaging Intelligence, Cambridge MA, USA</p>
    </td>
</tr>
<tr>
<td class="btn">
    <a href=""><svg style="width:36px;height:36px;margin-right:8px;margin-bottom:-9px" viewBox="0 0 24 24"><path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z"></path></svg>Paper</a>
    <a style="margin-left: 12pt;" href="https://colab.research.google.com/drive/1KqY0fTkDhuxvlXJeEPh_7viQYpl1wSe_?usp=sharing"><svg style="width:36px;height:36px;margin-right:8px;margin-bottom:-9px" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentColor" d="M16.9414 4.9757a7.033 7.033 0 0 0-4.9308 2.0646 7.033 7.033 0 0 0-.1232 9.8068l2.395-2.395a3.6455 3.6455 0 0 1 5.1497-5.1478l2.397-2.3989a7.033 7.033 0 0 0-4.8877-1.9297zM7.07 4.9855a7.033 7.033 0 0 0-4.8878 1.9316l2.3911 2.3911a3.6434 3.6434 0 0 1 5.0227.1271l1.7341-2.9737-.0997-.0802A7.033 7.033 0 0 0 7.07 4.9855zm15.0093 2.1721l-2.3892 2.3911a3.6455 3.6455 0 0 1-5.1497 5.1497l-2.4067 2.4068a7.0362 7.0362 0 0 0 9.9456-9.9476zM1.932 7.1674a7.033 7.033 0 0 0-.002 9.6816l2.397-2.397a3.6434 3.6434 0 0 1-.004-4.8916zm7.664 7.4235c-1.38 1.3816-3.5863 1.411-5.0168.1134l-2.397 2.395c2.4693 2.3328 6.263 2.5753 9.0072.5455l.1368-.1115z"/></svg>Colab (toy example)</a></td>
</tr>
</tbody>
</table>
</div>

<div class="content">
<h2>Overview</h2>
<table>
<tbody>
<tr style="text-align: center;">
    <td><video width="60%" muted autoplay loop><source src="images/pref-overview.mp4" type="video/mp4"></video></td>
</tr>
<tr>
    <td>Knowing the 3D motions in a dynamic scene is essential to many vision applications. Recent progress is mainly focused on estimating the activity of some specific elements like humans. In this paper, we leverage a neural motion field for estimating the motion of all points in a multiview setting. Modeling the motion from a dynamic scene with multiview data is challenging due to the ambiguities in points of similar color and points with time-varying color. We propose to regularize the estimated motion to be predictable. If the motion from previous frames is known, then the motion in the near future should be predictable. Therefore, we introduce a predictability regularization by first conditioning the estimated motion on latent embeddings, then by adopting a predictor network to enforce predictability on the embeddings. The proposed framework PREF (Predictability REgularized Fields) achieves on par or better results than state-of-the-art neural motion field-based dynamic scene representation methods, while requiring no prior knowledge of the scene.</td>
</tr>
<tr>
    <td><img src='images/pref-pipeline.png' width="100%"></td>
</tr>
</tbody>
</table>
</div>


<div class="content">
<h2>Video</h2>
<table>
<tbody>
<div class="video-container">
<iframe width="560" height="315" src="https://www.youtube.com/embed/HubW4HbhMTk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
</tbody>
</table>
</div>

<div class="content">
<h2>Citation</h2>
    <div width='100%' class="cite"> <pre><code>
@InProceedings{uii_eccv22_pref,
    title={PREF: Predictability Regularized Neural Motion Fields},
    author={Song, Liangchen and Gong, Xuan and Planche, Benjamin and Zheng, Meng 
            and Doermann, David and Yuan, Junsong and Chen, Terrence and Wu, Ziyan},
    booktitle={European Conference on Computer Vision},
    year={2022},
}
    </code></pre></div>
</div>


</div>
</body>
<script src="copy.js"></script>
<script src="../script.js"></script>
