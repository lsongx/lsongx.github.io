<!-- 
    Author: Liangchen Song
    Author URL: https://lsongx.github.io/
    License: Creative Commons Attribution 3.0 Unported
    License URL: http://creativecommons.org/licenses/by/3.0/
-->

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>NeRFPlayer</title>
    <meta name="author" content="Liangchen Song">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Bungee+Shade&family=Titillium+Web&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../style.css">
</head>


<body>
<div class="main" style="max-width: 1080px;">
<div class="content">
<table>
<tbody>
<tr style="text-align:center">
    <td style="font-size: 250%; font-weight: bold; width:1080px"><span class="rainbow">NeRFPlayer <svg style="margin-left: 5pt;" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" d="M12 0c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm-3 18v-12l10 6-10 6z"/></svg></span><br>
    <span style="line-height:100%; font-size: 80%; font-weight: bold">A Streamable Dynamic Scene Representation with<br>Decomposed Neural Radiance Fields</td>
</tr>
<tr style="text-align:center">
    <td>
        <p style="font-size: 120%; line-height:100%;">
        <a href="../index.html">Liangchen Song</a><sup style="color: #cb4b16; margin-right:0.4cm;">1</sup> 
        <a href="https://apchenstu.github.io">Anpei Chen</a><sup style="color: #859900;">2</sup><sup style="color: #d422a1; margin-right:0.4cm;">4</sup>
        <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a><sup style="color: #9d00ec; margin-right:0.4cm;">3</sup> 
        <a href="https://zhangchen8.github.io/">Zhang Chen</a><sup style="color: #9d00ec; margin-right:0.4cm;">3</sup>
        <a href="https://www.cs.rochester.edu/u/lchen63/">Lele Chen</a><sup style="color: #9d00ec; margin-right:0.4cm;">3</sup>
        </p>
        <p style="font-size: 120%; line-height:100%;">
        <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a><sup style="color: #cb4b16; margin-right:0.4cm;">1</sup> 
        <a href="https://www.linkedin.com/in/yi-xu-42654823/">Yi Xu</a><sup style="color: #9d00ec; margin-right:0.4cm;">3</sup>
        <a href="http://www.cvlibs.net">Andreas Geiger</a><sup style="color: #d422a1;">4</sup>
        </p>
        <p style="font-size: 90%;"><sup style="margin-left:0cm; color: #cb4b16;">1</sup> University at Buffalo
        <sup style="margin-left:0.5cm; color: #859900;">2</sup> ETH Zürich
        <sup style="margin-left:0.5cm; color: #9d00ec;">3</sup> OPPO US, InnoPeak Tech
        <sup style="margin-left:0.5cm; color: #d422a1;">4</sup> University of Tübingen</p>
    </td>
</tr>
<tr style="text-align:center">
<td class="btn">
    <a href="https://arxiv.org/pdf/2210.15947"><svg style="width:36px;height:36px;margin-right:8px;margin-bottom:-9px" viewBox="0 0 24 24"><path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z"></path></svg>Paper</a>
    <a style="margin-left: 12pt;" href="https://github.com/nerfstudio-project/nerfstudio"><svg xmlns="http://www.w3.org/2000/svg" style="width:36px;height:36px;margin-right:8px;margin-bottom:-9px" viewBox="0 0 24 24"><path fill="currentColor" d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>Code (nerfstudio version)</a>
    <!-- <a style="margin-left: 12pt;" href=""><svg xmlns="http://www.w3.org/2000/svg" style="width:36px;height:36px;margin-right:8px;margin-bottom:-9px" viewBox="0 0 24 24"><path fill="currentColor" d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>Code (coming soon)</a> -->
</td>
</tr>
</tbody>
</table>
</div>

<div class="content">
<h2 style="text-align:center">Video</h2>
<table>
<tbody>
<div class="video-container">
<iframe width="560" height="315" src="https://www.youtube.com/embed/flVqSLZWBMI" title="NeRFPlayer" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
</tbody>
</table>
</div>

<div class="content">
<h2 style="text-align:center">Overview</h2>
<table>
<tbody>
<tr>
    <td>Visually exploring in a real-world 4D spatiotemporal space freely in VR has been a long-term quest. The task is especially appealing when only a few or even single RGB cameras are used for capturing the dynamic scene. To this end, we present an efficient framework capable of fast reconstruction, compact modeling, and streamable rendering. First, we propose to decompose the 4D spatiotemporal space according to temporal characteristics. Points in the 4D space are associated with probabilities of belonging to three categories: <span style="color: #d4aa00;">static</span>, <span style="color: #2ca05c;">deforming</span>, and <span style="color: #0055d4;">new</span> areas. Each area is represented and regularized by a separate neural field. Second, we propose a hybrid representations based feature streaming scheme for efficiently modeling the neural fields. Our approach, coined NeRFPlayer, is evaluated on dynamic scenes captured by single hand-held cameras and multi-camera arrays, achieving comparable or superior rendering performance in terms of quality and speed comparable to recent state-of-the-art methods, achieving reconstruction in 10 seconds per frame and real-time rendering.</td>
</tr>
<tr style="text-align:center">
    <td><img src='images/nerfplayer-framework.png' width="100%"></td>
</tr>
</tbody>
</table>
</div>


<div class="content">
<h2 style="text-align:center">Citation</h2>
    <div width='100%' class="cite"> <pre><code>
@misc{song2022nerfplayer,
    title={NeRFPlayer: A Streamable Dynamic Scene Representation with Decomposed Neural Radiance Fields}, 
    author={Liangchen Song and Anpei Chen and Zhong Li and Zhang Chen and Lele Chen 
            and Junsong Yuan and Yi Xu and Andreas Geiger},
    year={2022},
    eprint={2210.15947},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
    </code></pre></div>
</div>


</div>
</body>
<script src="copy.js"></script>
<script src="../script.js"></script>
